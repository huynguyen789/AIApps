{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new functions for the search and summarize tool\n",
    "def clean_content(soup):\n",
    "    # Remove unnecessary elements\n",
    "    for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "        element.decompose()\n",
    "\n",
    "    # Try to find the main content area\n",
    "    main_content = (\n",
    "        soup.find('div', id='main-outlet') or  # Discourse forums\n",
    "        soup.find('main') or\n",
    "        soup.find('article') or\n",
    "        soup.find('div', class_='content') or\n",
    "        soup.find('div', id='content') or\n",
    "        soup.body  # Fallback to entire body if no specific content area found\n",
    "    )\n",
    "\n",
    "    if main_content:\n",
    "        # Extract text from relevant elements\n",
    "        content = []\n",
    "        for elem in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'blockquote', 'div', 'span']):\n",
    "            # Skip elements likely to contain metadata or navigation\n",
    "            if 'class' in elem.attrs and any(c in ['crawler-post-meta', 'topic-category', 'nav', 'menu'] for c in elem['class']):\n",
    "                continue\n",
    "            text = elem.get_text(strip=True)\n",
    "            if text:\n",
    "                content.append(text)\n",
    "\n",
    "        # Join the content\n",
    "        cleaned_content = '\\n\\n'.join(content)\n",
    "        # Remove extra whitespace\n",
    "        cleaned_content = re.sub(r'\\s+', ' ', cleaned_content).strip()\n",
    "        return cleaned_content\n",
    "    else:\n",
    "        return \"No main content found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.datacamp.com/tutorial/gpt4o-api-openai-tutorial\n",
    "#print out the content of the website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?None"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
